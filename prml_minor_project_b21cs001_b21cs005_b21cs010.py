# -*- coding: utf-8 -*-
"""Copy of PRML_minor_project_B21CS001_B21CS005_B21CS010.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18aroZP-iN4UsOgZf9n4m7gX7rokEefVR

B21CS001 :- A V Prithvi Kiran Naik
B21CS005 :- Akshat Jain
B21CS010 :- Ankana Chowdhury

Importing all the necessary libraries
"""

import pandas as pd
import numpy as np
from copy import deepcopy
import matplotlib.pyplot as plt
from sklearn.inspection import DecisionBoundaryDisplay
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

"""Mouting the google drive in google collab"""

from google.colab import drive
drive.mount('/content/drive')

"""Reading the dataset"""

data = pd.read_csv("/content/drive/MyDrive/Online_retail_utf.csv")

df_ = data

"""Changing the column names to lowercase and making a deepcopy of this dataset"""

df_.columns = [col.lower() for col in df_.columns]
df = deepcopy(df_)
df.head()

"""Preprocessing the dataset:

dropping rows with missing values
"""

df.dropna(inplace=True)

"""counting unique items"""

df['description'].nunique()

"""total number of each unique item"""

df['description'].value_counts()

"""Grouping description based on sum of its quantity and retrieving top 5 products with the highest total quantity."""

df.groupby('description').agg({'quantity': 'sum'}).sort_values(by='quantity', ascending=False).head()

"""Adding a new column total price in the dataframe and
Sorting dataframe in descending order based on values in the total price column
"""

df['total_price'] = df['quantity'] * df['unitprice']
df.sort_values(by='total_price', ascending=False)

"""Removing rows that represent cancellation or returns of previous transaction"""

df = df[~df['invoiceno'].str.contains('C', na=False)]

"""Count of unique countries and total count of each country in the dataset"""

print(df['country'].nunique())
print(df['country'].value_counts())

"""Grouping customers based on each country they belong to"""

df3 = df.groupby('customerid')['country'].value_counts().reset_index(name='count')
df3

print(df3['country'].nunique())
print(df3['country'].value_counts())

"""Number of unique invoice numbers"""

print(df['invoiceno'].nunique())

"""Number of times unique customers reappeared


"""

df['customerid'].value_counts()

"""grouping customers based on their total invoice numbers"""

df4 = df.groupby('customerid')['invoiceno'].value_counts().reset_index(name='count')
df4

"""Total number of reappearance of each customer"""

print(df4['customerid'].value_counts())

"""Importing all the neccesary libraries"""

from sklearn.cluster import KMeans
from scipy.spatial import Voronoi, voronoi_plot_2d
import datetime as dt
from datetime import datetime
from sklearn.decomposition import PCA

"""Total monetary value of purchases of each customer"""

x1 = df.groupby('customerid').agg({'total_price': 'sum'})

"""Total items bought by the customers"""

x2 = df.groupby('customerid').agg({'quantity': 'sum'})

"""Number of times the buyer visited"""

x3 = df.groupby('customerid').agg({'invoiceno': 'nunique'})

"""Sorting x3 by invoice number in descending order"""

x3.sort_values(by = 'invoiceno',ascending = False)

"""Converting the date string values in the 5th column of df into datetime objects"""

for i in range(len(df)):
  df.iloc[i,4] = datetime.strptime(df.iloc[i,4], '%d-%m-%Y %H:%M')

"""Maximum of the invoicedate column of dataframe df"""

df['invoicedate'].max()

"""Cheking the type"""

type(df['invoicedate'].max())

"""assigning the reference date"""

reference_date = dt.datetime(2011,12,16)
reference_date

"""assigning x4 as recency of the purchases by the customer"""

x4 = df.groupby('customerid').agg({'invoicedate': lambda x: (reference_date - x.max()).days})

"""Creating a dataframe d2 using x1,x2,x3 and x4 as columns"""

df2 = pd.DataFrame()
df2['x1'] = x1
df2['x2'] = x2
df2['x3'] = x3
df2['x4'] = x4

"""printing df2"""

df2

"""Removing rows with negative values from the respective columns"""

rows_dropped = []
col_n = list(df2.columns)

for index,row in df2.iterrows():
  for col in col_n:
    if row[col] <= 0:
      rows_dropped.append(index)

print(rows_dropped)
df2 = df2.drop(list(set(rows_dropped)), axis=0, inplace=False)

"""Creating scatterplot and visualizing total bill against total number of products"""

plt.scatter(df2.iloc[:,0],df2.iloc[:,1],s =2)
plt.xlabel('Total Bill')
plt.ylabel('Total No. of Products')
plt.show()

"""Plotting total bill against frequency of buying"""

plt.scatter(df2.iloc[:,0],df2.iloc[:,2],s =2)
plt.xlabel('Total Bill')
plt.ylabel('Frequency of buying')
plt.show()

"""creating a new dataframe y5"""

y1 = df.groupby('customerid').agg({'total_price': 'sum'}).reset_index()
y2 = df.groupby('customerid').agg({'quantity': 'sum'}).reset_index()
y3 = df.groupby('customerid').agg({'invoiceno': 'nunique'}).reset_index()
y4 = df.groupby('customerid').agg({'invoicedate': lambda x: (reference_date - x.max()).days}).reset_index()
y5 = pd.DataFrame()
y5['customerid'] = y1['customerid']
y5['total_spending'] = y1['total_price']
y5['total_items'] = y2['quantity']
y5['frequency'] = y3['invoiceno']
y5['recency'] = y4['invoicedate']
print(y5)

"""Applying kmeans to the dataframe df2 and
finding the best value of k(optimum cluster number) using the elbow method
"""

kmeans = KMeans()
ssd = []
K = range(1, 30)
for k in K:
    kmeans = KMeans(n_clusters=k,n_init='auto').fit(df2)
    ssd.append(kmeans.inertia_)

plt.plot(K, ssd, 'bx-')
plt.xlabel('SSE/SSR/SSD values for different K values')
plt.title('Elbow method for the optimum cluster number')
plt.show(block=True)

"""Again visualizing using the elbow method"""

from yellowbrick.cluster import KElbowVisualizer
model = KMeans(n_init='auto')
visualizer = KElbowVisualizer(model, k=(2,100))

visualizer.fit(df2)        # Fit the data to the visualizer
visualizer.show()

"""Plotting the results(clusters) using a scatterplot for total bill vs total no. of products."""

model = KMeans(n_clusters=11, random_state=0,n_init='auto')
labels = model.fit_predict(df2)
u_labels = np.unique(labels)

#plotting the results:

for i in u_labels:
    plt.scatter(df2[labels == i].iloc[:,0] , df2[labels == i].iloc[:,1], label = i,s = 4)
plt.xlabel('Total Bill')
plt.ylabel('Total No. of Products')
plt.legend()
plt.show()

"""Finding cluster centers"""

centers=model.cluster_centers_
new_centers = centers[:,:2]

"""Plotting Voronoi plots"""

vor = Voronoi(new_centers)
fig = voronoi_plot_2d(vor,plt.gca())

plt.show()

fig = voronoi_plot_2d(vor,plt.gca())
for i in u_labels:
    plt.scatter(df2[labels == i].iloc[:,0] , df2[labels == i].iloc[:,1], label = i,s = 4)

"""Plotting the results(clusters) using a scatterplot for total bill vs frequency of buying"""

for i in u_labels:
    plt.scatter(df2[labels == i].iloc[:,0] , df2[labels == i].iloc[:,2], label = i,s = 4)
plt.xlabel('Total Bill')
plt.ylabel('Frequency of buying')
plt.legend()
plt.show()

"""Finding shape of the centers"""

centers.shape

"""assigning new centres"""

a1 = centers[:,0].reshape(-1,1)
a2 = centers[:,2].reshape(-1,1)
new_centers = np.hstack((a1,a2))
new_centers.shape

"""Making Voronoi plots using the new centers"""

vor = Voronoi(new_centers)
fig = voronoi_plot_2d(vor,plt.gca())

plt.show()

fig = voronoi_plot_2d(vor,plt.gca())
for i in u_labels:
    plt.scatter(df2[labels == i].iloc[:,0] , df2[labels == i].iloc[:,2], label = i,s = 4)

"""Applying PCA to dataframe df2"""

pca = PCA(n_components = 4)
pca.fit(df2)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel("Number of components")
plt.ylabel("Cumulative Explained Variance")
plt.xlim(0,5)
plt.show()

"""Applying PCA with n_components=2 and applying kmeans and also using elbow method"""

pca = PCA(n_components = 2)
d1 = pca.fit_transform(df2)
d1 = pd.DataFrame(d1)
print(d1)
model = KMeans(n_init='auto')
visualizer = KElbowVisualizer(model, k=(2,30))

visualizer.fit(d1)        # Fit the data to the visualizer
visualizer.show()

"""Plotting the clusters"""

model = KMeans(n_clusters=7, random_state=0,n_init='auto')
labels = model.fit_predict(d1)
u_labels = np.unique(labels)

#plotting the results:

for i in u_labels:
    plt.scatter(d1[labels == i].iloc[:,0] , d1[labels == i].iloc[:,1], label = i,s = 4)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

"""Making Voronoi plots"""

centers=model.cluster_centers_
vor = Voronoi(centers)
fig = voronoi_plot_2d(vor,plt.gca())
plt.show()

fig = voronoi_plot_2d(vor,plt.gca())
for i in u_labels:
    plt.scatter(d1[labels == i].iloc[:,0] , d1[labels == i].iloc[:,1], label = i,s = 4)

"""Creating a RFM(recency, frequency, monetary) dataframe as rfm"""

rfm = pd.DataFrame()
rfm['recency'] = y5['recency']
rfm['frequency'] = y5['frequency']
rfm['monetary'] = y5['total_spending']
rfm['customerid'] = y5['customerid']

"""Description of rfm"""

rfm.describe().T

"""Checking if monetary is 0 anywhere or not in rfm"""

rfm = rfm[rfm['monetary'] > 0]

"""Calculating recency score, monetary score, frequency score"""

rfm['recency_score'] = pd.qcut(rfm['recency'], 3, labels=[ 3, 2, 1])

rfm['monetary_score'] = pd.qcut(rfm['monetary'], 3, labels=[1, 2, 3])

rfm['frequency_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 3, labels=[1, 2, 3])

"""Calculating aggragate rfm score"""

rfm['rfm_score'] = rfm['recency_score'].astype(str) + rfm['monetary_score'].astype(str) + rfm['frequency_score'].astype(str)
rfm.head()

"""Sorting values of rfm by rfm_score"""

rfm.sort_values(by = 'rfm_score', ascending = False)

"""Visualizing monetary, frequency, recency using histplots"""

plt.figure(figsize=(20, 5))
plt.subplot(1, 3, 1)
sns.histplot(rfm['monetary'])
plt.subplot(1, 3, 2)
sns.histplot(rfm['frequency'])
plt.subplot(1, 3, 3)
sns.histplot(rfm['recency'])
plt.show(block=True)

"""min max scaling the data"""

rfm_k = pd.DataFrame()
rfm_k['recency'] = rfm['recency']
rfm_k['frequency'] = rfm['frequency']
rfm_k['monetary'] = rfm['monetary']
print(rfm_k)
mms = MinMaxScaler().fit_transform(rfm_k)
rfm_scaled = pd.DataFrame(mms)

"""Describing the RFM data"""

rfm_scaled.describe().T

"""Applying K-means clustering after doing dimentionality reduction with PCA"""

pca = PCA(n_components = 2)
d2 = pca.fit_transform(rfm_scaled)
d2 = pd.DataFrame(d2)
d2['customerid'] = rfm['customerid']
print(d2)
model = KMeans(n_init='auto')
visualizer = KElbowVisualizer(model, k=(2,30))

visualizer.fit(d2.iloc[:,:2])        # Fit the data to the visualizer
visualizer.show()

"""Plotting the data along with clusters for optimum number of clusters"""

model = KMeans(n_clusters=8, random_state=0,n_init='auto')
labels = model.fit_predict(d2.iloc[:,:2])
d2['k_means_labels'] = labels
u_labels = np.unique(labels)

#plotting the results:

for i in u_labels:
    plt.scatter(d2[labels == i].iloc[:,0] , d2[labels == i].iloc[:,1], label = i,s = 4)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

"""Plotting the Vorinoi plots seperately and with data"""

centers=model.cluster_centers_
vor = Voronoi(centers)
fig = voronoi_plot_2d(vor,plt.gca())
plt.show()

fig = voronoi_plot_2d(vor,plt.gca())
for i in u_labels:
    plt.scatter(d2[labels == i].iloc[:,0] , d2[labels == i].iloc[:,1], label = i,s = 4)
plt.xlim([-0.3,1])
plt.ylim([-0.1,1.2])

print(d2)

"""Output Cluster for k-means clustering with the help of the RFM matrix of the original retail data."""

output_k_means = pd.DataFrame()
output_k_means['customerid'] = d2['customerid']
output_k_means['labels'] = d2['k_means_labels']
print(output_k_means)

"""Implementing Heirchail Clustering"""

import scipy.cluster.hierarchy as shc

plt.figure(figsize=(10, 7))
plt.title("Dendrogram")

# Selecting Annual Income and Spending Scores by index
selected_data = rfm_k.iloc[:,:2]
clusters = shc.linkage(selected_data,
            method='ward',
            metric="euclidean")
shc.dendrogram(Z=clusters)
plt.show()

"""Calculating silhouette score for heirchial clustering"""

import numpy as np
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import fcluster

optimal_clusters = 3

hierarchical_labels = fcluster(clusters , optimal_clusters , criterion = 'maxclust')

hierarchical_silhoutte = silhouette_score(selected_data , hierarchical_labels)
print('Agglomerative Hierarchical Clustering Silhouette Score:' , hierarchical_silhoutte)